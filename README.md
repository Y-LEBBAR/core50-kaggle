COMP 432 â€“ CORE50 Kaggle Project

Simple MLP Baseline Classifier for Object Recognition

This repository contains the full codebase for our COMP 432 Kaggle competition submission.
The goal is to build a baseline model that classifies images in the CORE50 dataset using a simple Multilayer Perceptron (MLP).
All code is modular, fully commented, and designed to be improved later with deeper models.

## 1. Project Overview

We implement a baseline MLP classifier trained on flattened image vectors.
This model is intentionally simple (no convolution layers yet).
Its purpose is to:

establish a strong, clean baseline (~52% validation accuracy)

create a modular codebase for teammates

allow fast experimentation

run both locally and on Google Colab

This baseline already achieves:

Best validation accuracy: 0.5213 (52.13%) (0.517 on kaggle submission)

which surpasses the 50% threshold required for the competition baseline.

## 2. Repository Structure
core50-kaggle/
â”‚
â”œâ”€â”€ data/                     # NOT pushed to GitHub (.gitignore)
â”‚     â”œâ”€â”€ train.csv
â”‚     â”œâ”€â”€ test.csv
â”‚     â””â”€â”€ sample_submission.csv
â”‚
â”œâ”€â”€ models/
â”‚     â”œâ”€â”€ best_model.pth      # Saved best PyTorch model
â”‚     â””â”€â”€ mean_std.npz        # Normalization statistics
â”‚
â”œâ”€â”€ notebooks/
â”‚     â””â”€â”€ Comp_432_project.ipynb  # Google Colab training notebook
â”‚
â”œâ”€â”€ src/
â”‚     â”œâ”€â”€ config.py           # Global configuration
â”‚     â”œâ”€â”€ data.py             # Dataset loading + preprocessing
â”‚     â”œâ”€â”€ model.py            # MLP model definition
â”‚     â”œâ”€â”€ train.py            # Training loop
â”‚     â”œâ”€â”€ predict.py          # Inference + submission generation
â”‚     â””â”€â”€ utils.py            # Helpers (normalization, saving model, etc.)
â”‚
â”œâ”€â”€ submissions/
â”‚     â””â”€â”€ submission.csv      # Kaggle submission file
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

## 3. Explanation of Each File
### ğŸ“ src/config.py

Central configuration file. Defines:

batch size

learning rate

number of epochs

model size

file paths

This makes it easy to change settings across the entire project.

### ğŸ“ src/data.py

Handles:

loading train.csv and test.csv

extracting input features and labels

applying standardization (mean/std)

splitting dataset into 80/20 train/validation

Output: PyTorch Dataset and DataLoader objects.

### ğŸ“ src/model.py

Defines the MLP classifier:

Input â†’ Linear â†’ ReLU â†’ Dropout â†’ Linear â†’ Output

Fully connected network

No convolution layers (simple baseline)

The model is intentionally small and fast.

### ğŸ“ src/train.py

Main training loop:

loads data

trains model for 25 epochs

tracks training loss & validation accuracy

saves the best model automatically as models/best_model.pth

saves normalization stats (mean_std.npz)

Final validation result achieved:

Val Accuracy: 0.5213

### ğŸ“ src/predict.py

Used to generate submission.csv:

loads the saved best model

loads test.csv

normalizes using training mean/std

outputs predictions in the exact Kaggle format

saves them to submissions/submission.csv

### ğŸ“ src/utils.py

Utility functions including:

saving/loading model

saving/loading normalization stats

accuracy computation

small helper wrappers

### ğŸ“ notebooks/Comp_432_project.ipynb

A full Google Colab notebook that walks through:

cloning repo

installing dependencies

uploading train/test/sample CSVs

running training (train.py)

running prediction (predict.py)

downloading submission.csv

uploading it to GitHub manually

This notebook is clean, ready for teammates, and contains all steps.

## 4. Running the Project Locally
### 1. Create virtual environment
python -m venv .venv
.venv\Scripts\activate

### 2. Install dependencies
pip install -r requirements.txt

### 3. Add CSV files

Place your dataset in:

data/train.csv
data/test.csv
data/sample_submission.csv

### 4. Train the model
python src/train.py

### 5. Generate submission
python src/predict.py


Output will be saved to:

submissions/submission.csv

## 5. Running With Google Colab

Use the notebook:

notebooks/Comp_432_project.ipynb


It performs:

repo cloning

environment setup

file uploads

training

prediction

downloading submission

Colab is recommended for faster CPU/GPU training.

## 6. Current Results

Our simple MLP baseline achieved:

Best validation accuracy: 0.5213

This satisfies the competition requirement and provides a strong benchmark.

Submission file contains:

49,460 predictions


Matching the exact expected format.

## 7. Next Steps: Improving Accuracy
ğŸ”¥ 1. Switch from MLP â†’ Convolutional Neural Network (CNN)

MLP ignores spatial structure.
Even a small ConvNet (2 conv layers) will likely reach 70â€“80% accuracy.

ğŸ”¥ 2. Add data augmentation

Random:

flips

brightness shifts

cropping

rotation

This reduces overfitting and increases validation accuracy.

ğŸ”¥ 3. Use a deeper MLP

Add:

more layers

batch normalization

increased hidden units

ğŸ”¥ 4. Early stopping + learning rate scheduler

Improves stability.

ğŸ”¥ 5. Feature scaling using PCA

Dimensionality reduction before MLP may help.

ğŸ”¥ 6. Replace MLP with Logistic Regression / SVM baseline

For comparison.

ğŸ”¥ 7. Use PyTorch Lightning for cleaner training code
## 8. Team Workflow Recommendations

Use GitHub for collaboration

Keep .gitignore strict (no data pushed)

Use Colab for training

All team members should run the notebook to reproduce results

Every new model should be stored as a separate script inside src/models/

## 9. Credits

Team members: Yannis Lebbar + collaborators
Course: COMP 432 â€“ Machine Learning
Dataset: CORE50 (Kaggle Competition)